---
- name: Setup kube-vip floating ip
  hosts: control_plane
  become: true
  tasks:
    - name: Ensure jq and curl are installed
      ansible.builtin.package:
        name: "{{ item }}"
        state: present
      loop:
        - jq
        - curl

    - name: Pull kube-vip image using containerd
      ansible.builtin.shell: |
        ctr image pull ghcr.io/kube-vip/kube-vip:{{ lookup('ansible.builtin.env', 'KUBE_VIP_VERSION') }}

    - name: Ensure /etc/kubernetes/manifests directory exists
      ansible.builtin.file:
        path: /etc/kubernetes/manifests
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Set kube-vip environment variables
      ansible.builtin.set_fact:
        kube_vip_interface: "{{ lookup('ansible.builtin.env', 'KUBE_VIP_INTERFACE') }}"
        kube_vip_ip: "{{ lookup('ansible.builtin.env', 'KUBE_VIP_IP') }}"

    - name: Generate kube-vip static pod manifest for control plane with ARP
      ansible.builtin.shell: |
        ctr run --rm --net-host ghcr.io/kube-vip/kube-vip:{{ lookup('ansible.builtin.env', 'KUBE_VIP_VERSION') }} vip /kube-vip manifest pod \
        --interface {{ kube_vip_interface }} \
        --address {{ kube_vip_ip }} \
        --controlplane \
        --arp \
        --enableNodeLabeling \
        --leaderElection | tee /etc/kubernetes/manifests/kube-vip.yaml
      register: manifest_output

    - name: Replace admin.conf with super-admin.conf in kube-vip manifest
      ansible.builtin.replace:
        path: /etc/kubernetes/manifests/kube-vip.yaml
        regexp: "path: /etc/kubernetes/admin.conf"
        replace: "path: /etc/kubernetes/super-admin.conf"

    - name: Display kube-vip manifest creation output
      ansible.builtin.debug:
        var: manifest_output.stdout

    - name: Copy kube-vip manifest to all control plane nodes
      ansible.builtin.copy:
        src: /etc/kubernetes/manifests/kube-vip.yaml
        dest: /etc/kubernetes/manifests/kube-vip.yaml
        mode: "0644"
        owner: root
        group: root
      when: inventory_hostname != groups['control_plane'][0]

- hosts: control_plane
  name: Initialize a new kubernetes cluster from control plane node
  become: true

  tasks:
    - name: Pulling images required for setting up a Kubernetes cluster
      ansible.builtin.command: kubeadm config images pull

    - name: Download runc binary
      ansible.builtin.get_url:
        url: https://github.com/opencontainers/runc/releases/download/{{ lookup('ansible.builtin.env', 'RUNC_VERSION') }}/runc.amd64
        dest: /tmp/runc.amd64
        mode: "0755"

    - name: Install runc binary
      ansible.builtin.command:
        cmd: install -m 755 /tmp/runc.amd64 /usr/local/sbin/runc

    - name: Run kubeadm init with control plane endpoint
      ansible.builtin.shell: |
        kubeadm init \
        --apiserver-advertise-address={{ hostvars[inventory_hostname]['ansible_host'] }} \
        --apiserver-cert-extra-sans={{ lookup('ansible.builtin.env', 'TAILSCALE_JUMP_SERVER_VPN_IP') }} \
        --pod-network-cidr={{ lookup('ansible.builtin.env', 'POD_NETWORK_CIDR') }} \
        --control-plane-endpoint={{ lookup('ansible.builtin.env', 'KUBE_VIP_IP') }} \
        --v=5
      args:
        executable: /bin/bash
      register: myshell_output

    - ansible.builtin.debug:
        msg: '"{{ myshell_output.stdout }}"'

    - name: Create .kube directory
      become: true
      become_user: admin
      ansible.builtin.file:
        path: $HOME/.kube
        state: directory
        mode: "0755"

    - name: Copy admin.conf to user's kube config
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/admin/.kube/config
        remote_src: true
        owner: admin

- hosts: control_plane
  name: Setup Calico CNI
  tasks:
    - name: Check if Kubernetes cluster is running
      ansible.builtin.command: kubectl cluster-info
      register: cluster_info
      ignore_errors: true

    - name: Print cluster status
      ansible.builtin.debug:
        var: cluster_info

    - name: Create Calico Operator using kubectl
      ansible.builtin.command: kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/{{ lookup('ansible.builtin.env', 'CALICO_VERSION') }}/manifests/tigera-operator.yaml
        --validate=false
      register: tigera_operator_output

    - name: Download Calico Custom Resources manifest
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/projectcalico/calico/{{ lookup('ansible.builtin.env', 'CALICO_VERSION') }}/manifests/custom-resources.yaml
        dest: /tmp/custom-resources.yaml

    - name: Change cidr value in the custom-resources.yaml
      ansible.builtin.replace:
        path: /tmp/custom-resources.yaml
        regexp: "cidr: 192\\.168\\.0\\.0/16"
        replace: "cidr: {{ lookup('ansible.builtin.env', 'POD_NETWORK_CIDR') }}"

    - name: Apply Custom Resources manifest using kubectl
      ansible.builtin.command: kubectl apply -f /tmp/custom-resources.yaml
      register: calico_custom_resources_output

    - name: Ensure Tigera Operator was created successfully
      ansible.builtin.debug:
        msg: "{{ tigera_operator_output.stdout }}"

    - name: Ensure Calico Custom Resources applied successfully
      ansible.builtin.debug:
        msg: "{{ calico_custom_resources_output.stdout }}"

#
#- hosts: control_plane
#  name: Update kube-vip to use admin instead of super admin
#  become: true
#  tasks:
#
#    - name: Replace super-admin.conf with admin.conf in kube-vip manifest
#      ansible.builtin.replace:
#        path: /etc/kubernetes/manifests/kube-vip.yaml
#        regexp: 'path: /etc/kubernetes/super-admin.conf'
#        replace: 'path: /etc/kubernetes/admin.conf'
